"""
Full LLM Swiggy Ordering Agent - All responses generated by AI
"""
import logging
from typing import Dict, Any, Optional, List
from app.models.schemas import AssistantResponse, ResponseType, UIElement
from app.services.in_memory_state_manager import in_memory_state_manager
from app.services.ai_service import AIService
import base64

logger = logging.getLogger(__name__)


class FullLLMSwiggyAgent:
    """Full LLM agent for Swiggy ordering - all responses generated by AI"""
    
    def __init__(self):
        self.swiggy_steps = [
            "Open the Swiggy app on your phone. If you don't see the Swiggy app, say 'I don't see the Swiggy app' and I'll help you find it.",
            "Search for the food you want to order. A simple search like 'Pizza' or 'Biriyani' will work. If you don't see the search bar, say 'I don't see the search bar' and I'll help you find it.",
            "Select a restaurant from the results. Always check the rating of the restaurant before selecting it. If you don't find the restaurant, say 'I don't see the restaurant' and I'll help you find it.",
            "Choose the items you want to order. If you don't find the item, say 'I don't see the item' and I'll help you find it.",
            "Add items to your cart. When clicked the Add button, you may asked to customize the item by adding toppings, sauces, etc. If you don't want to customize, just click Add Item button without selecting anything.",
            "Once you have added all the items you want to order, clickon the 'View Cart' button. If you don't see the 'View Cart' button, say 'I don't see the View Cart button' and I'll help you find it.",
            "Once you are in the cart page, a pop up might appear with Instant discount offer. You can click on the 'YAY' button to apply the discount. If you don't need the discount, just click on the 'X' button at the top right corner of the pop up.",
            "Review the items in your card and Click on the 'Proceed to Pay' button.",
            "Now you will be in the Payment options page where all the payment options will be listed. You can select the payment option you want to use. If you don't see the payment option you want to use, say 'I don't see the payment option' and I'll help you find it.",
            "Once you have found the desired payment option, click over the payment option. You will be redirected to the payment page where you can enter your payment details. If you don't see the payment page, say 'I don't see the payment page' and I'll help you find it.",
            "You have successfully placed your order. You can see the order in the 'Orders' tab. If you don't see the order, say 'I don't see the order' and I'll help you find it.",
        ]
        self.ai_service = AIService()
    
    async def process_request(self, session_id: str, user_input: str) -> AssistantResponse:
        """Process user request with full LLM responses"""
        logger.info(f"ü§ñ [FULL_LLM_AGENT] ===== Processing request =====")
        logger.info(f"ü§ñ [FULL_LLM_AGENT] Session ID: {session_id}")
        logger.info(f"ü§ñ [FULL_LLM_AGENT] User input: '{user_input}'")
        logger.info(f"ü§ñ [FULL_LLM_AGENT] Input length: {len(user_input)} characters")
        
        try:
            # Load existing state from Redis
            existing_state = await in_memory_state_manager.load_workflow_state(session_id)
            
            if existing_state:
                current_step = existing_state.get('current_step_index', 0)
                workflow_status = existing_state.get('workflow_status', 'unknown')
                logger.info(f"üìÇ [FULL_LLM_AGENT] Loaded existing state:")
                logger.info(f"üìÇ [FULL_LLM_AGENT]   - Current step: {current_step}")
                logger.info(f"üìÇ [FULL_LLM_AGENT]   - Workflow status: {workflow_status}")
                logger.info(f"üìÇ [FULL_LLM_AGENT]   - Workflow type: {existing_state.get('workflow_type', 'none')}")
                return await self._handle_existing_workflow_llm(session_id, user_input, existing_state)
            else:
                logger.info(f"üÜï [FULL_LLM_AGENT] No existing state found - starting new workflow")
                return await self._start_new_workflow_llm(session_id, user_input)
                
        except Exception as e:
            logger.error(f"‚ùå [FULL_LLM_AGENT] Error in process_request: {e}")
            logger.error(f"‚ùå [FULL_LLM_AGENT] Error type: {type(e).__name__}")
            return self._get_error_response(session_id, str(e))
    
    async def _start_new_workflow_llm(self, session_id: str, user_input: str) -> AssistantResponse:
        """Start a new Swiggy ordering workflow with LLM-generated response"""
        logger.info(f"üÜï [FULL_LLM_AGENT] Starting new workflow with LLM for session: {session_id}")
        
        # Check if user wants to order food using LLM
        is_swiggy_request = await self._is_swiggy_request_llm(user_input)
        logger.info(f"üÜï [FULL_LLM_AGENT] LLM determined is Swiggy request: {is_swiggy_request}")
        
        if not is_swiggy_request:
            logger.info(f"üÜï [FULL_LLM_AGENT] Not a Swiggy request - generating LLM guidance")
            response_content = await self._generate_guidance_response(user_input)
            return AssistantResponse(
                session_id=session_id,
                response_type=ResponseType.INSTRUCTION,
                content=response_content,
                ui_element=None,
                next_step=None
            )
        
        # Start workflow with LLM-generated welcome
        logger.info(f"üÜï [FULL_LLM_AGENT] Creating new workflow state")
        workflow_state = {
            "session_id": session_id,
            "user_input": user_input,
            "workflow_type": "swiggy_order_food",
            "workflow_steps": self.swiggy_steps,
            "current_step_index": 0,
            "workflow_status": "active",
            "waiting_for_verification": True,
            "workflow_context": {},
            "interruption_handled": False,
            "conversation_history": [{"role": "user", "content": user_input}]
        }
        logger.info(f"üÜï [FULL_LLM_AGENT] Workflow state created with {len(self.swiggy_steps)} steps")
        
        # Save to Redis
        logger.info(f"üÜï [FULL_LLM_AGENT] Saving workflow state to Redis")
        await in_memory_state_manager.save_workflow_state(session_id, workflow_state)
        logger.info(f"üÜï [FULL_LLM_AGENT] Workflow state saved successfully")
        
        # Generate first step with LLM
        current_step = self.swiggy_steps[0]
        response_content = await self._generate_step_response(current_step, 0, len(self.swiggy_steps), "starting")
        logger.info(f"üÜï [FULL_LLM_AGENT] Generated LLM first step response")
        
        response = AssistantResponse(
            session_id=session_id,
            response_type=ResponseType.INSTRUCTION,
            content=response_content,
            ui_element=None,
            next_step=None
        )
        logger.info(f"üÜï [FULL_LLM_AGENT] Returning LLM-generated response")
        return response
    
    async def _handle_existing_workflow_llm(self, session_id: str, user_input: str, state: Dict[str, Any]) -> AssistantResponse:
        """Handle existing workflow state with full LLM responses"""
        current_step_index = state.get('current_step_index', 0)
        workflow_status = state.get('workflow_status', 'idle')
        workflow_type = state.get('workflow_type', 'unknown')
        
        logger.info(f"üîÑ [FULL_LLM_AGENT] ===== Handling existing workflow with LLM =====")
        logger.info(f"üîÑ [FULL_LLM_AGENT] Current step: {current_step_index}")
        logger.info(f"üîÑ [FULL_LLM_AGENT] Workflow status: {workflow_status}")
        logger.info(f"üîÑ [FULL_LLM_AGENT] Workflow type: {workflow_type}")
        
        # Update conversation history
        self._update_conversation_history(state, user_input, "user")
        logger.info(f"üîÑ [FULL_LLM_AGENT] Updated conversation history with {len(state.get('conversation_history', []))} messages")
        
        # Use LLM to determine user intent
        user_intent = await self._analyze_user_intent_llm(user_input, state)
        logger.info(f"üîÑ [FULL_LLM_AGENT] LLM determined user intent: {user_intent}")
        
        if user_intent == "verification":
            logger.info(f"üîÑ [FULL_LLM_AGENT] Handling step verification with LLM")
            return await self._handle_step_verification_llm(session_id, user_input, state)
        elif user_intent == "interruption":
            logger.info(f"üîÑ [FULL_LLM_AGENT] Handling interruption with LLM")
            return await self._handle_interruption_llm(session_id, user_input, state)
        elif user_intent == "new_workflow":
            logger.info(f"üîÑ [FULL_LLM_AGENT] Starting new workflow")
            return await self._start_new_workflow_llm(session_id, user_input)
        else:
            logger.info(f"üîÑ [FULL_LLM_AGENT] Generating contextual response with LLM")
            return await self._generate_contextual_response_llm(session_id, user_input, state)
    
    # ===== CONVERSATION HISTORY MANAGEMENT =====
    
    def _update_conversation_history(self, state: Dict[str, Any], content: str, role: str):
        """Update conversation history in the state"""
        if 'conversation_history' not in state:
            state['conversation_history'] = []
        
        # Add new message
        state['conversation_history'].append({"role": role, "content": content})
        
        # Keep only last 10 messages to avoid token limits
        if len(state['conversation_history']) > 10:
            state['conversation_history'] = state['conversation_history'][-10:]
    
    def _get_conversation_context(self, state: Dict[str, Any]) -> str:
        """Get formatted conversation history for LLM context"""
        history = state.get('conversation_history', [])
        if not history:
            return "No previous conversation."
        
        context_parts = []
        for msg in history[-5:]:  # Last 5 messages
            role = "User" if msg['role'] == 'user' else "Assistant"
            context_parts.append(f"{role}: {msg['content']}")
        
        return "\n".join(context_parts)
    
    # ===== NEW LLM-BASED METHODS =====
    
    async def _is_swiggy_request_llm(self, user_input: str) -> bool:
        """Use LLM to determine if user wants to order from Swiggy"""
        prompt = f"""Analyze this user input and determine if they want to order food from Swiggy.

User input: "{user_input}"

Respond with only "YES" if they want to order food from Swiggy, or "NO" if they don't.

Examples:
- "I want to order food from Swiggy" ‚Üí YES
- "Order food" ‚Üí YES  
- "Swiggy order" ‚Üí YES
- "Hello" ‚Üí NO
- "What's the weather?" ‚Üí NO
- "Help me with something else" ‚Üí NO

Response:"""

        try:
            response = await self.ai_service.generate_text(
                prompt=prompt,
                model="gpt-4",
                max_tokens=10,
                temperature=0.1
            )
            result = response.strip().upper() == "YES"
            logger.info(f"ü§ñ [FULL_LLM_AGENT] _is_swiggy_request_llm('{user_input}') = {result}")
            return result
        except Exception as e:
            logger.error(f"‚ùå [FULL_LLM_AGENT] Error in _is_swiggy_request_llm: {e}")
            # Fallback to keyword matching
            return self._is_swiggy_request(user_input)
    
    async def _analyze_user_intent_llm(self, user_input: str, state: Dict[str, Any]) -> str:
        """Use LLM to analyze user intent with conversation context and better step tracking"""
        current_step_index = state.get('current_step_index', 0)
        workflow_steps = state.get('workflow_steps', self.swiggy_steps)
        current_step = workflow_steps[current_step_index] if current_step_index < len(workflow_steps) else "Unknown"
        conversation_context = self._get_conversation_context(state)
        
        # Add more context about the current step
        step_context = f"Step {current_step_index + 1} of {len(workflow_steps)}: {current_step}"
        if current_step_index > 0:
            previous_step = workflow_steps[current_step_index - 1]
            step_context += f" (Previous step was: {previous_step})"
        
        prompt = f"""You are analyzing user intent for an elderly user ordering food from Swiggy. Be very careful about step ordering.

CONVERSATION HISTORY:
{conversation_context}

Current workflow context: {step_context}

User input: "{user_input}"

IMPORTANT: Only respond with one of these exact options:
- "verification" - if they're confirming they completed the CURRENT step (done, next, completed, finished, yes, ok, ready, continue, I did it, finished this step)
- "interruption" - if they're asking a question or need help (how, what, where, why, when, help, explain, show, tell me, confused, stuck, problem, I don't understand)
- "new_workflow" - if they want to start a completely new Swiggy order (start over, new order, begin again)
- "general" - for greetings, small talk, or unrelated messages

Examples:
- "done" ‚Üí verification
- "I finished this step" ‚Üí verification  
- "How do I open the app?" ‚Üí interruption
- "I want to order food" ‚Üí new_workflow (only if they want to start over)
- "Hello" ‚Üí general
- "Thank you" ‚Üí general

Be very careful: Only mark as "verification" if they're clearly confirming the CURRENT step is done.

Response:"""

        try:
            response = await self.ai_service.generate_text(
                prompt=prompt,
                model="gpt-4",
                max_tokens=20,
                temperature=0.1
            )
            intent = response.strip().lower()
            logger.info(f"ü§ñ [FULL_LLM_AGENT] _analyze_user_intent_llm('{user_input}') = {intent}")
            return intent
        except Exception as e:
            logger.error(f"‚ùå [FULL_LLM_AGENT] Error in _analyze_user_intent_llm: {e}")
            # Fallback to keyword matching
            if self._is_verification_response(user_input):
                return "verification"
            elif self._is_interruption(user_input):
                return "interruption"
            elif self._is_swiggy_request(user_input):
                return "new_workflow"
            else:
                return "general"
    
    async def _generate_guidance_response(self, user_input: str) -> str:
        """Generate guidance response for non-Swiggy requests"""
        prompt = f"""You are a helpful assistant for elderly users. The user has sent a message that's not related to ordering food from Swiggy.

User input: "{user_input}"

Respond in 2-3 short sentences maximum:
1. Acknowledge their message briefly
2. Explain you help with Swiggy food ordering
3. Offer to help

Use simple, clear language. Be polite but very concise.

Response:"""

        try:
            response = await self.ai_service.generate_text(
                prompt=prompt,
                model="gpt-4",
                max_tokens=60,
                temperature=0.7
            )
            logger.info(f"ü§ñ [FULL_LLM_AGENT] Generated guidance response")
            return response.strip()
        except Exception as e:
            logger.error(f"‚ùå [FULL_LLM_AGENT] Error generating guidance response: {e}")
            return "Hello! I'm here to help you order food from Swiggy. Just say 'I want to order food from Swiggy' and I'll guide you through each step."
    
    async def _generate_step_response(self, current_step: str, step_index: int, total_steps: int, context: str) -> str:
        """Generate step instruction with LLM"""
        prompt = f"""You are a helpful assistant for elderly users ordering food from Swiggy.

Current step: {current_step}
Step number: {step_index + 1} of {total_steps}
Context: {context}

Create a clear instruction in 2-3 short sentences maximum:
1. Give the main instruction clearly
2. Ask them to confirm when done

Use simple, clear language. Be polite but very concise.

Response:"""

        try:
            response = await self.ai_service.generate_text(
                prompt=prompt,
                model="gpt-4",
                max_tokens=80,
                temperature=0.7
            )
            logger.info(f"ü§ñ [FULL_LLM_AGENT] Generated step response for step {step_index + 1}")
            return response.strip()
        except Exception as e:
            logger.error(f"‚ùå [FULL_LLM_AGENT] Error generating step response: {e}")
            return f"Step {step_index + 1} of {total_steps}: {current_step}\n\nPlease let me know when you've completed this step by saying 'done' or 'next'."
    
    async def _handle_step_verification_llm(self, session_id: str, user_input: str, state: Dict[str, Any]) -> AssistantResponse:
        """Handle step verification with LLM-generated responses and better step tracking"""
        current_step_index = state.get('current_step_index', 0)
        workflow_steps = state.get('workflow_steps', self.swiggy_steps)
        
        logger.info(f"‚úÖ [FULL_LLM_AGENT] ===== Handling step verification with LLM =====")
        logger.info(f"‚úÖ [FULL_LLM_AGENT] Current step index: {current_step_index}")
        logger.info(f"‚úÖ [FULL_LLM_AGENT] Current step: {workflow_steps[current_step_index] if current_step_index < len(workflow_steps) else 'Unknown'}")
        logger.info(f"‚úÖ [FULL_LLM_AGENT] Total steps: {len(workflow_steps)}")
        
        # Validate current step index
        if current_step_index >= len(workflow_steps):
            logger.error(f"‚ùå [FULL_LLM_AGENT] Invalid step index: {current_step_index} >= {len(workflow_steps)}")
            return self._get_error_response(session_id, "Invalid workflow state")
        
        # Advance to next step
        next_step_index = current_step_index + 1
        
        if next_step_index >= len(workflow_steps):
            # Workflow completed
            logger.info(f"üéâ [FULL_LLM_AGENT] ===== Workflow completed! =====")
            
            # Clear workflow state
            await in_memory_state_manager.delete_workflow_state(session_id)
            logger.info(f"üéâ [FULL_LLM_AGENT] Workflow state cleared")
            
            # Generate completion response with LLM
            completion_response = await self._generate_completion_response()
            
            return AssistantResponse(
                session_id=session_id,
                response_type=ResponseType.INSTRUCTION,
                content=completion_response,
                ui_element=None,
                next_step=None
            )
        else:
            # Continue to next step
            current_step = workflow_steps[next_step_index]
            
            # Update state
            state['current_step_index'] = next_step_index
            state['workflow_status'] = 'active'
            state['waiting_for_verification'] = True
            
            # Save to Redis
            await in_memory_state_manager.save_workflow_state(session_id, state)
            logger.info(f"‚úÖ [FULL_LLM_AGENT] State saved successfully")
            
            # Generate next step response with LLM
            context = f"continuing from step {current_step_index + 1} to step {next_step_index + 1}"
            response_content = await self._generate_step_response(current_step, next_step_index, len(workflow_steps), context)
            
            # Add assistant response to conversation history
            self._update_conversation_history(state, response_content, "assistant")
            
            response = AssistantResponse(
                session_id=session_id,
                response_type=ResponseType.INSTRUCTION,
                content=response_content,
                ui_element=None,
                next_step=None
            )
            logger.info(f"‚úÖ [FULL_LLM_AGENT] Returning LLM-generated next step response")
            return response
    
    async def _handle_interruption_llm(self, session_id: str, user_input: str, state: Dict[str, Any]) -> AssistantResponse:
        """Handle interruption with LLM-generated responses"""
        current_step_index = state.get('current_step_index', 0)
        workflow_steps = state.get('workflow_steps', self.swiggy_steps)
        current_step = workflow_steps[current_step_index] if current_step_index < len(workflow_steps) else "Unknown"
        
        logger.info(f"üö® [FULL_LLM_AGENT] ===== Handling interruption with LLM =====")
        logger.info(f"üö® [FULL_LLM_AGENT] User question: '{user_input}'")
        logger.info(f"üö® [FULL_LLM_AGENT] Current step: {current_step_index + 1} - {current_step}")
        
        # Save interruption to Redis
        interruption_data = {
            "type": "workflow_interruption",
            "user_input": user_input,
            "workflow_type": "swiggy_order_food",
            "current_step_index": current_step_index,
            "current_step": current_step,
            "total_steps": len(workflow_steps),
            "workflow_status": state.get('workflow_status', 'unknown')
        }
        
        await in_memory_state_manager.save_interruption(session_id, interruption_data)
        logger.info(f"üö® [FULL_LLM_AGENT] Interruption saved successfully")
        
        # Generate contextual response using LLM
        try:
            response = await self._generate_interruption_response(user_input, current_step, current_step_index, workflow_steps, state)
            logger.info(f"ü§ñ [FULL_LLM_AGENT] Generated LLM interruption response")
        except Exception as e:
            logger.error(f"‚ùå [FULL_LLM_AGENT] Error generating LLM response: {e}")
            response = f"I understand you have a question about: '{user_input}'\n\nYou're currently on Step {current_step_index + 1}: {current_step}\n\nI'm here to help! What would you like to know about this step?"
        
        # Add assistant response to conversation history
        self._update_conversation_history(state, response, "assistant")
        
        return AssistantResponse(
            session_id=session_id,
            response_type=ResponseType.INSTRUCTION,
            content=response,
            ui_element=None,
            next_step=None
        )
    
    async def _generate_completion_response(self) -> str:
        """Generate completion response with LLM"""
        prompt = """You are a helpful assistant for elderly users. The user has successfully completed all steps of ordering food from Swiggy.

Generate a polite completion message in 2-3 short sentences maximum:
1. Congratulate them briefly
2. Confirm their order is on the way
3. Offer help if needed

Use simple, clear language. Be polite but very concise.

Response:"""

        try:
            response = await self.ai_service.generate_text(
                prompt=prompt,
                model="gpt-4",
                max_tokens=60,
                temperature=0.7
            )
            logger.info(f"ü§ñ [FULL_LLM_AGENT] Generated completion response")
            return response.strip()
        except Exception as e:
            logger.error(f"‚ùå [FULL_LLM_AGENT] Error generating completion response: {e}")
            return "Great! You've completed all the steps successfully. Your Swiggy order should be on its way to you. Is there anything else I can help you with?"
    
    async def _generate_contextual_response_llm(self, session_id: str, user_input: str, state: Dict[str, Any]) -> AssistantResponse:
        """Generate contextual response for general input"""
        current_step_index = state.get('current_step_index', 0)
        workflow_steps = state.get('workflow_steps', self.swiggy_steps)
        current_step = workflow_steps[current_step_index] if current_step_index < len(workflow_steps) else "Unknown"
        conversation_context = self._get_conversation_context(state)
        
        prompt = f"""You are a helpful assistant for elderly users. The user has sent a general message during a Swiggy ordering workflow.

CONVERSATION HISTORY:
{conversation_context}

Current step: {current_step_index + 1} of {len(workflow_steps)} - {current_step}
User input: "{user_input}"

Generate a helpful response in 2-3 short sentences maximum:
1. Acknowledge their message briefly
2. Guide them back to the current step
3. Offer to help

Use simple, clear language. Be polite but very concise.

Response:"""

        try:
            response = await self.ai_service.generate_text(
                prompt=prompt,
                model="gpt-4",
                max_tokens=60,
                temperature=0.7
            )
            logger.info(f"ü§ñ [FULL_LLM_AGENT] Generated contextual response")
            
            # Add assistant response to conversation history
            self._update_conversation_history(state, response.strip(), "assistant")
            
            return AssistantResponse(
                session_id=session_id,
                response_type=ResponseType.INSTRUCTION,
                content=response.strip(),
                ui_element=None,
                next_step=None
            )
        except Exception as e:
            logger.error(f"‚ùå [FULL_LLM_AGENT] Error generating contextual response: {e}")
            return AssistantResponse(
                session_id=session_id,
                response_type=ResponseType.INSTRUCTION,
                content="I'm here to help you with your Swiggy order. Please say 'done' when you complete a step, or ask me any questions you might have. I'm patient and happy to help!",
                ui_element=None,
                next_step=None
            )
    
    async def _generate_interruption_response(self, user_question: str, current_step: str, current_step_index: int, workflow_steps: List[str], state: Dict[str, Any]) -> str:
        """Generate contextual response using LLM for user questions during workflow"""
        logger.info(f"ü§ñ [FULL_LLM_AGENT] ===== Generating LLM response =====")
        logger.info(f"ü§ñ [FULL_LLM_AGENT] User question: '{user_question}'")
        logger.info(f"ü§ñ [FULL_LLM_AGENT] Current step: {current_step_index + 1} - {current_step}")
        logger.info(f"ü§ñ [FULL_LLM_AGENT] Total workflow steps: {len(workflow_steps)}")
        
        # Create context for the LLM
        workflow_context = "\n".join([f"{i+1}. {step}" for i, step in enumerate(workflow_steps)])
        conversation_context = self._get_conversation_context(state)
        logger.info(f"ü§ñ [FULL_LLM_AGENT] Created workflow context with {len(workflow_steps)} steps")
        
        prompt = f"""You are a helpful assistant for elderly users ordering food from Swiggy. The user is currently on step {current_step_index + 1} of {len(workflow_steps)}.

CONVERSATION HISTORY:
{conversation_context}

Current Step: {current_step}

Complete Workflow:
{workflow_context}

User's Question: "{user_question}"

Please provide a helpful, specific answer in 2-3 short sentences maximum:
1. Answer their question directly
2. Give practical guidance
3. Keep them focused on the current step

Use simple, clear language. Be polite but very concise.

Response:"""

        logger.info(f"ü§ñ [FULL_LLM_AGENT] Prompt length: {len(prompt)} characters")
        logger.info(f"ü§ñ [FULL_LLM_AGENT] Calling OpenAI API with gpt-4")

        try:
            response = await self.ai_service.generate_text(
                prompt=prompt,
                model="gpt-4",
                max_tokens=80,
                temperature=0.7
            )
            logger.info(f"ü§ñ [FULL_LLM_AGENT] OpenAI API response received")
            logger.info(f"ü§ñ [FULL_LLM_AGENT] Response length: {len(response)} characters")
            logger.info(f"ü§ñ [FULL_LLM_AGENT] Response preview: {response[:100]}...")
            return response.strip()
        except Exception as e:
            logger.error(f"‚ùå [FULL_LLM_AGENT] Error generating LLM response: {e}")
            logger.error(f"‚ùå [FULL_LLM_AGENT] Error type: {type(e).__name__}")
            raise e
    
    # Fallback methods for keyword matching
    def _is_swiggy_request(self, user_input: str) -> bool:
        """Check if user wants to order from Swiggy"""
        swiggy_keywords = ['swiggy', 'order food', 'order from swiggy', 'food delivery', 'swiggy order']
        user_lower = user_input.lower()
        result = any(keyword in user_lower for keyword in swiggy_keywords)
        logger.debug(f"üîç [FULL_LLM_AGENT] _is_swiggy_request('{user_input}') = {result}")
        return result
    
    def _is_verification_response(self, user_input: str) -> bool:
        """Check if user is confirming a step"""
        verification_responses = ['done', 'next', 'completed', 'finished', 'yes', 'ok', 'ready', 'continue']
        user_lower = user_input.lower().strip()
        result = any(response in user_lower for response in verification_responses)
        logger.debug(f"üîç [FULL_LLM_AGENT] _is_verification_response('{user_input}') = {result}")
        return result
    
    def _is_interruption(self, user_input: str) -> bool:
        """Check if user is asking a question"""
        # First check for verification responses - these are NOT interruptions
        if self._is_verification_response(user_input):
            logger.debug(f"üîç [FULL_LLM_AGENT] _is_interruption('{user_input}') = False (verification response)")
            return False
        
        # Then check for interruption keywords
        interruption_keywords = [
            'how', 'what', 'where', 'why', 'when', 'which', 'can you', 'could you',
            'help', 'explain', 'show', 'tell me', 'i don\'t understand', 'confused',
            'stuck', 'problem', 'error', 'not working', 'can\'t find', 'difficult'
        ]
        
        user_lower = user_input.lower()
        result = any(keyword in user_lower for keyword in interruption_keywords)
        logger.debug(f"üîç [FULL_LLM_AGENT] _is_interruption('{user_input}') = {result}")
        return result
    
    def _get_error_response(self, session_id: str, error_message: str) -> AssistantResponse:
        """Get error response"""
        logger.error(f"‚ùå [FULL_LLM_AGENT] Creating error response for session: {session_id}")
        logger.error(f"‚ùå [FULL_LLM_AGENT] Error message: {error_message}")
        return AssistantResponse(
            session_id=session_id,
            response_type=ResponseType.ERROR,
            content=f"I'm sorry, I encountered an error: {error_message}. Please try again.",
            ui_element=None,
            next_step=None
        )
    
    # ===== IMAGE PROCESSING METHODS =====
    
    async def process_request_with_image(self, session_id: str, user_input: str, image_base64: str) -> AssistantResponse:
        """Process user request with image analysis"""
        logger.info(f"üì∏ [FULL_LLM_AGENT] ===== Processing request with image =====")
        logger.info(f"üì∏ [FULL_LLM_AGENT] Session ID: {session_id}")
        logger.info(f"üì∏ [FULL_LLM_AGENT] User input: '{user_input}'")
        logger.info(f"üì∏ [FULL_LLM_AGENT] Image size: {len(image_base64)} characters")
        
        try:
            # First analyze the image
            image_analysis = await self.analyze_screenshot(image_base64)
            logger.info(f"üì∏ [FULL_LLM_AGENT] Image analysis completed")
            
            # Combine user input with image analysis
            combined_input = f"{user_input}\n\n[Screenshot Analysis: {image_analysis}]"
            
            # Process the combined input through normal workflow
            response = await self.process_request(session_id, combined_input)
            
            # Add image context to the response
            response.content = f"üì∏ I can see your screenshot! {response.content}"
            
            logger.info(f"üì∏ [FULL_LLM_AGENT] Image-enhanced response generated")
            return response
            
        except Exception as e:
            logger.error(f"‚ùå [FULL_LLM_AGENT] Error processing request with image: {e}")
            return self._get_error_response(session_id, str(e))
    
    async def analyze_screenshot(self, image_base64: str) -> str:
        """Analyze screenshot using LLM vision capabilities"""
        logger.info(f"üîç [FULL_LLM_AGENT] ===== Analyzing screenshot =====")
        logger.info(f"üîç [FULL_LLM_AGENT] Image size: {len(image_base64)} characters")
        
        try:
            # Create prompt for image analysis
            prompt = """You are a helpful assistant for elderly users with technology. 

I need you to analyze this screenshot of a mobile phone screen. Please provide a comprehensive analysis that helps an elderly user understand what they're seeing and what to do next.

IMPORTANT: Pay special attention to ALL text content in the image, including:
- App names, titles, and headers
- Button labels and menu items
- Error messages or notifications
- Form fields and input prompts
- Status messages or confirmations
- Any popup dialogs or alerts

Provide a concise analysis in 2-3 short sentences maximum:
1. **What you see**: Identify the app/screen and read any important text
2. **What to do**: Give clear next steps
3. **Any issues**: Mention problems if any

Use simple, clear language. Be polite but very concise. Focus on the most important information.

Please analyze this screenshot and provide helpful guidance:"""

            # Use OpenAI Vision API for image analysis
            from app.config import settings
            response = await self.ai_service.analyze_image(
                prompt=prompt,
                image_base64=image_base64,
                model=settings.vision_model,
                max_tokens=120,
                temperature=0.7
            )
            
            logger.info(f"üîç [FULL_LLM_AGENT] Image analysis completed")
            logger.info(f"üîç [FULL_LLM_AGENT] Analysis length: {len(response)} characters")
            return response.strip()
            
        except Exception as e:
            logger.error(f"‚ùå [FULL_LLM_AGENT] Error analyzing screenshot: {e}")
            logger.error(f"‚ùå [FULL_LLM_AGENT] Error type: {type(e).__name__}")
            
            # Try fallback with different model
            try:
                logger.info(f"üîÑ [FULL_LLM_AGENT] Trying fallback with gpt-4o-mini")
                response = await self.ai_service.analyze_image(
                    prompt=prompt,
                    image_base64=image_base64,
                    model="gpt-4o-mini",
                    max_tokens=200,
                    temperature=0.7
                )
                logger.info(f"üîÑ [FULL_LLM_AGENT] Fallback analysis successful")
                return response.strip()
            except Exception as fallback_error:
                logger.error(f"‚ùå [FULL_LLM_AGENT] Fallback also failed: {fallback_error}")
                return "I can see your screenshot, but I'm having trouble analyzing it right now. I'm still here to help you with your Swiggy order. Please describe what you see on your screen, and I'll guide you through it step by step."


# Global instance
full_llm_swiggy_agent = FullLLMSwiggyAgent()
